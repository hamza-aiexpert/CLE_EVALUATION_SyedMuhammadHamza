{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## SYED MUHAMMAD HAMZA\n",
    "# Problem 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49acb963edb12c60"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Function to read Urdu stopwords from a file\n",
    "def read_stopwords(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = [word.strip() for word in f]\n",
    "    return stopwords\n",
    "\n",
    "# Function to tokenize text from sentences into words by splitting on the basis of whitespaces\n",
    "def tokenize_text(text):\n",
    "    return text.split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:30.470279900Z",
     "start_time": "2024-03-04T17:22:30.443282800Z"
    }
   },
   "id": "30d1e04fa5d73c9d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read Urdu stopwords\n",
    "stopwords = read_stopwords('stopwords.txt')\n",
    "\n",
    "# Read text file with Urdu content\n",
    "with open('FOR_TS_ISF.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = tokenize_text(text)\n",
    "\n",
    "# Remove Urdu stopwords by comparing the words with the words in stopwords.txt file\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "# Find unique words in the text\n",
    "unique_words = list(set(words))\n",
    "\n",
    "# Count the number of sentences each word appears in\n",
    "sentences_word_count = {word: 0 for word in unique_words}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:31.137379300Z",
     "start_time": "2024-03-04T17:22:31.123381300Z"
    }
   },
   "id": "f537867ac3081456"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.جیسے', 'آپکی', 'مرضی', '.تم', 'کیا', 'کرنے', 'والے', 'ہو', '.عقل', 'کی', 'بات', 'کرو', '.آپ', 'میری', 'بات', 'نہیں', 'سمجھ', 'رہے', '.ایک', 'دن', 'چھوڑ', 'کر', 'آیا', 'کرو', '.چھوٹی', 'چھوٹی', 'باتوں', 'پر', 'مت', 'جھگڑو', '.یہ', 'بلکل', 'فضول', 'بات', 'ہے', '.وہ', 'بہت', 'باتونی', 'ہے', '.کچھ', 'گڑبڑ', 'ہے', '.بڑے', 'ہی', 'مفت', 'خور', 'ہو', '.گد', 'گدی', 'کرنا', 'بند', 'کرو', '.کوئی', 'بات', 'نہیں', '.میں', 'اب', 'زیادہ', 'دیر', 'انتظار', 'نہیں', 'کر', 'سکتا', '.تمہیں', 'میرے', 'ساتھ', 'کام', 'کرنا', 'ہوگا', '.وہ', 'تیز', 'تیز', 'چلتی', 'ہے', '.مجھےاردو', 'اچھی', 'لگتی', 'ہے', '.میں', 'ایک', 'مہینہ', 'سے', 'اردو', 'سیکھرہاہوں', '.پاکستان', 'بہت', 'اچھا', 'ملک', 'ہے', '.اللہ', 'کا', 'فضل', 'ہو', '.مجھے', 'اردو', 'کی', 'مَشق', 'کرنی', 'چاہیے']\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(words)\n",
    "print(len(words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:31.805284400Z",
     "start_time": "2024-03-04T17:22:31.786288300Z"
    }
   },
   "id": "c356d67987d1c2bc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مہینہ', 'والے', '.عقل', '.پاکستان', 'باتونی', 'ہے', '.گد', '.مجھےاردو', 'خور', 'زیادہ', 'سے', 'سیکھرہاہوں', 'اردو', 'ملک', 'ایک', '.جیسے', 'کرو', 'کرنا', 'مرضی', 'کر', 'بند', 'چھوڑ', '.کچھ', 'کیا', '.ایک', 'کا', 'بلکل', 'فضل', 'ہی', 'اب', 'میری', 'لگتی', 'انتظار', 'چلتی', 'اچھا', '.آپ', 'کرنے', '.وہ', 'بات', '.کوئی', 'ساتھ', 'چاہیے', 'رہے', '.اللہ', 'سکتا', 'سمجھ', 'جھگڑو', '.تمہیں', '.تم', '.مجھے', 'آپکی', 'گڑبڑ', 'نہیں', 'دن', 'دیر', 'باتوں', 'میرے', 'چھوٹی', 'کرنی', 'مت', 'مفت', 'گدی', '.چھوٹی', 'کام', '.بڑے', 'ہو', 'ہوگا', 'آیا', 'کی', 'فضول', '.یہ', 'اچھی', '.میں', 'مَشق', 'پر', 'بہت', 'تیز']\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "print(unique_words)\n",
    "print(len(unique_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:32.397030100Z",
     "start_time": "2024-03-04T17:22:32.379032700Z"
    }
   },
   "id": "fb29b987f3d7676e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'جیسے آپکی مرضی\\n', 'تم کیا کرنے والے ہو\\n', 'عقل کی بات کرو\\n', 'آپ میری بات نہیں سمجھ رہے\\n', 'ایک دن چھوڑ کر آیا کرو\\n', 'چھوٹی چھوٹی باتوں پر مت جھگڑو\\n', 'یہ بلکل فضول بات ہے\\n', 'وہ بہت باتونی ہے\\n', 'کچھ گڑبڑ ہے\\n', 'بڑے ہی مفت خور ہو\\n', 'گد گدی کرنا بند کرو\\n', 'کوئی بات نہیں\\n', 'میں اب زیادہ دیر انتظار نہیں کر سکتا\\n', 'تمہیں میرے ساتھ کام کرنا ہوگا\\n', 'وہ تیز تیز چلتی ہے\\n', 'مجھےاردو اچھی لگتی ہے\\n', 'میں ایک مہینہ سے اردو سیکھرہاہوں \\n', 'پاکستان بہت اچھا ملک ہے\\n', 'اللہ کا فضل ہو\\n', 'مجھے اردو کی مَشق کرنی چاہیے']\n"
     ]
    }
   ],
   "source": [
    "print(text.split('.'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:33.054421500Z",
     "start_time": "2024-03-04T17:22:33.023426100Z"
    }
   },
   "id": "479e9808814c933a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split('.')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:33.801688600Z",
     "start_time": "2024-03-04T17:22:33.782690Z"
    }
   },
   "id": "fdd5f03808cc0fee"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# As described in the Problem statement TF-ISF(W,S)=log(TF*ISF)\n",
    "# Function to calculate TF-ISF value\n",
    "def calculate_tf_isf(tf, isf):\n",
    "    return math.log(tf * isf)\n",
    "\n",
    "# Function to calculate TF-ISF score of a sentence\n",
    "def calculate_sentence_tf_isf(sentence, unique_words, sentences_word_count):\n",
    "    sentence_tf_isf = {}\n",
    "    sentence_words = tokenize_text(sentence)\n",
    "    total_sentences = len(sentences_word_count)\n",
    "\n",
    "    for word in unique_words:\n",
    "        if word in sentence_words:\n",
    "            # Simply counting words in a sentence\n",
    "            tf = sentence_words.count(word)\n",
    "            # Calculating Inverse Document Frequency \n",
    "            isf = total_sentences / sentences_word_count[word]\n",
    "            sentence_tf_isf[word] = calculate_tf_isf(tf, isf)\n",
    "        else:\n",
    "            sentence_tf_isf[word] = 0.0\n",
    "\n",
    "    return sentence_tf_isf\n",
    "\n",
    "# Function to calculate TF-ISF score of a sentence\n",
    "def calculate_score(sentence_tf_isf):\n",
    "    return sum(sentence_tf_isf.values())\n",
    "\n",
    "for sentence in text.split('.'):\n",
    "    sentence_words = tokenize_text(sentence)\n",
    "    for word in set(sentence_words):\n",
    "        if word in sentences_word_count:\n",
    "            sentences_word_count[word] += 1\n",
    "\n",
    "# Compute TF-ISF values of words for each sentence\n",
    "tf_isf_output = \"Word\\t\" + \"\\t\".join(unique_words) + \"\\n\"\n",
    "sentence_tf_isf_scores = []\n",
    "\n",
    "# I added . at the end of eah sentence in the txt file to make it easy for me to split the text in sentences\n",
    "for sentence in text.split('.'):\n",
    "    sentence_no = len(sentence_tf_isf_scores) + 1\n",
    "    tf_isf_output += f\"Sentence {sentence_no}\\t\"\n",
    "    sentence_tf_isf = calculate_sentence_tf_isf(sentence, unique_words, sentences_word_count)\n",
    "    sentence_tf_isf_scores.append(calculate_score(sentence_tf_isf))\n",
    "    for word in unique_words:\n",
    "        tf_isf_output += f\"{sentence_tf_isf[word]}\\t\"\n",
    "    tf_isf_output += \"\\n\"\n",
    "\n",
    "# Saving TS_ISF values in txt file \n",
    "with open('tf_isf.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(tf_isf_output)\n",
    "\n",
    "# Write TF-ISF scores of sentences to file\n",
    "sentence_scores_output = \"Sentence No.\\tTF-ISF Score\\n\"\n",
    "for i, score in enumerate(sentence_tf_isf_scores):\n",
    "    sentence_scores_output += f\"Sentence {i+1}\\t{score}\\n\"\n",
    "\n",
    "# Saving the sentences scores in txt file\n",
    "with open('sentence_scores.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sentence_scores_output)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T17:22:34.418496500Z",
     "start_time": "2024-03-04T17:22:34.401533100Z"
    }
   },
   "id": "589db2b46a8a249b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "494bb2abd5940bf2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
